# -*- coding: utf-8 -*- d
"""1_MERGING.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1kpaIQpsIqwcJzrzn-Do8kFDOK8biFdkt
"""

#https://github.com/vivianhylee/high-dynamic-range-image

#paulmatteschk@googlemail.com
#gagandeepduklu@gmail.com

#CAMERA SENSOR SONY IMX477 SRGGB12
#FISHEYE LENS 185° ALL SKY IMAGES
#INPUT EXPOSURE SERIES/STACK WITH 9 FRAMES
#EXPOSURETIMES = exp_time = [50,100,150,250,400,650,1050,1700,2750] µs (1e-6s)
#the resultign hdr pixel values are proportional to the true radiance values in the scene
#https://people.eecs.berkeley.edu/~malik/papers/debevec-malik97.pdf

import numpy as np
from numpy import asarray
import cv2
import random
import glob
import os
from natsort import natsorted
from PIL import Image
import pandas as pd
import statistics
from skimage import exposure, img_as_ubyte
from PIL import ImageFile
ImageFile.LOAD_TRUNCATED_IMAGES = True
from datetime import datetime
import pickle
import matplotlib.pyplot as plt

from google.colab import drive
drive.mount('/content/drive')

def get_sky_area(radius, centre, img):
    img = Image.open(str(image_name))
    cv2_img = np.array(img)
    cv2_img = cv2.cvtColor(cv2_img, cv2.COLOR_RGB2BGR)
    
    sky = cv2_img.copy()
    # create a black image with the same size as our 
    # image that contains the moon, we then create
    # a white circle on the black image
    mask = np.zeros(sky.shape[:2], dtype="uint8")
    cv2.circle(mask, centre, radius, 255, -1)
    # apply the mask to our image
    masked = cv2.bitwise_and(sky, sky, mask=mask)
    avgR = np.mean(masked[:,:,0])
    avgG = np.mean(masked[:,:,1])
    avgB = np.mean(masked[:,:,2])
    print("Mean of channel R: ", avgR)
    print("Mean of channel G: ", avgG)
    print("MEan of channel B: ", avgB)
    
    #cv2.imshow("sky", sky)
    #cv2.imshow("mask", mask)
    #cv2.imwrite('sky_image.png', masked[:,:,2])
    print("Saved Sky image as sky_image")
    #cv2.imshow("Mask applied to image", masked)
    #cv2.waitKey()
    return avgR, avgG, avgB
    
'''    
CENTRE= (574, 335)
RADIUS = 353   
IMAGE_NAME = "1669190042_1700.tga"
avgR, avgG, avgB = get_sky_area(radius = RADIUS, centre = CENTRE, image_name=IMAGE_NAME)


rggb_sky=circled(r,c)

avg = np.average(rggb_sky)
print(avg)
'''

1014/600

760/450

def resize_and_mask(img, width, height, radius, centre): 
  w, h, _ = img.shape
  dims = (width, height)
  resized = cv2.resize(img, dims, interpolation = cv2.INTER_AREA)
  return resized


# resized = resize_image(img, 0.5)
def get_mean_ind(img_arr, channel):
  """
  image_arr = array that contains all the images
  channel   = channel whose average shall be considered

  Returns
    mean_ind = index of the image closest to the mean
    mean     = value of the mean
    tl       = list of the means for respective imagees for given channel
  """
  tl = [img_arr[i,:,:,channel].mean() for i in range(9)]        #avg chanelwise imagewise
  mean = np.mean(tl)                                            #avg([avg chanelwise imagewise])
  mean_ind = np.argmin(np.abs(np.array(tl) - np.mean(tl)))      #index from exposure layer with most average average exposure (r,g,b)

  return mean_ind, mean, tl

def plot_list_bar_all(channel_list_data, exp_time, timestamp):

  """
  Plots the mean of the bar for chunk of images for all 3 channels

  channel_list_data = List of list containing information 
                        list of means for given channel,   mean of the image for given channel, index of image closest to the mean   
                        
  exp_time          = List of exposure time in mili seconds
  timestamp         = Timestamp at which all images were captured, used as title of the plot
  """

# Extract all information : list of mean for individual image, mean of all images and index closes to to the mean
  list0, mean0, ind_0 = channel_list_data[0] # channel 0 
  list1, mean1, ind_1 = channel_list_data[1] # channel 1 
  list2, mean2, ind_2 = channel_list_data[2] # channel 2 

  bar_width = 0.2 # width of the bar

  color_dict = {} # Dictionary to manage colors in the plot,  index 0 = lighter, index 1 = darker

#OFFICIAL UNIVERSITY COLORS
  color_dict["red"] = ["#FF0000", "#CB1F1F"]
  color_dict["blue"] = ["#3970E5", "#042670"]
  color_dict["green"] = ["#07E651", "#056B27"]


  # First bar position
  red_positions0 = [x + bar_width for x in range(len(list0))]


  # Shift the second bar's position on the x-axis
  green_positions1 = [x + bar_width for x in red_positions0]

  # Shift the third bar's position on the x-axis
  blue_positions2 = [x + bar_width for x in green_positions1]

  # Plot all bars from respective lists 
  plt.bar(red_positions0, list0, bar_width, color=color_dict["red"][1])
  plt.bar(green_positions1, list1, bar_width, color=color_dict["green"][1])
  plt.bar(blue_positions2, list2, bar_width, color=color_dict["blue"][1])

  # Plot the specefic bars that are closest to the mean
  plt.bar(red_positions0[ind_0], list0[ind_0], bar_width, color=color_dict["red"][0])
  plt.bar(green_positions1[ind_1], list1[ind_1], bar_width, color=color_dict["green"][0])
  plt.bar(blue_positions2[ind_2], list2[ind_2], bar_width, color=color_dict["blue"][0])

 # Plot the lines for the mean
  plt.axhline(y=np.mean(list0), color=color_dict["red"][0])
  plt.axhline(y=np.mean(list1), color=color_dict["green"][0])
  plt.axhline(y=np.mean(list2), color=color_dict["blue"][0])

# Set the ticker text of x axis
  x_axis = [str(exp_time[i]) for i in range(len(exp_time))]
  plt.xticks(green_positions1, x_axis)

# Plotting 
  plt.ylabel("Iluminance Value")
  plt.xlabel("Exposure Time \u03BCs")
  # plt.gca().xaxis.get_major_ticks()[min_index]
  plt.title("Time stamp "+str(timestamp))
  plt.show()



def plot_list_bar(list_float, exp_time, min_index, timestamp):
  plt.bar(range(len(list_float)), list_float)
  plt.bar(min_index, list_float[min_index], color='green')
  plt.axhline(y=np.mean(list_float), color='green')

  for i, v in enumerate(list_float):
    plt.text(i, v, str(exp_time[i]), color='blue', fontweight='bold', ha='center', va='bottom')

  plt.xticks(range(len(list_float)))
  plt.ylabel("Mean Value")
  plt.xlabel("Image Index")
  plt.title(str(ts))
  plt.show()


def save_dict(file_name, dict_obj):
  with open(file_name, 'wb') as handle:
      pickle.dump(dict_obj, handle, protocol=pickle.HIGHEST_PROTOCOL)

def read_dict(file_name):
  with open(file_name, 'rb') as handle:
      dict_obj = pickle.load(handle)
      return dict_obj




#####################################################################################################################################
#BETRAG MITTELWERT 
#AVERAGE IMAGE INTENSITY
def linearWeight(pixel_value):
    """ Linear weighting function based on pixel intensity that reduces the
    weight of pixel values that are near saturation.

    Parameters
    ----------
    pixel_value : np.uint8
        A pixel intensity value from 0 to 255

    Returns
    -------
    weight : np.float64
        The weight corresponding to the input pixel intensity

    """
    z_min, z_max = 0., 255. #ADJUST BIT SRGGB12
    if pixel_value <= (z_min + z_max) / 2:
        return pixel_value - z_min
    return z_max - pixel_value

#stichprobe um exposure layer einzuordnen --sorting by brightness
def sampleIntensities(images):
    """
    Randomly sample pixel intensities from the exposure stack.

    Parameters
    ----------
    images : list<numpy.ndarray>
        A list containing a stack of single-channel (i.e., grayscale)
        layers of an HDR exposure stack

    Returns
    -------
    intensity_values : numpy.array, dtype=np.uint8
        An array containing a uniformly sampled intensity value from each
        exposure layer (shape = num_intensities x num_images)#exposure layer

    """
    z_min, z_max = 0, 255
    num_intensities = z_max - z_min + 1
    num_images = len(images)
    intensity_values = np.zeros((num_intensities, num_images), dtype=np.uint8)  #(1,9)

    # Find the middle image to use as the source for pixel intensity locations
    mid_img = images[num_images // 2]

    for i in range(z_min, z_max + 1):
        rows, cols = np.where(mid_img == i)
        if len(rows) != 0:
            idx = random.randrange(len(rows))
            for j in range(num_images):
                intensity_values[i, j] = images[j][rows[idx], cols[idx]]
    return intensity_values


def computeResponseCurve(intensity_samples, log_exposures, smoothing_lambda, weighting_function):
    """Find the camera response curve for a single color channel

    Parameters
    ----------
    intensity_samples : numpy.ndarray
        Stack of single channel input values (num_samples x num_images)

    log_exposures : numpy.ndarray
        Log exposure times (size == num_images)

    smoothing_lambda : float
        A constant value used to correct for scale differences between
        data and smoothing terms in the constraint matrix -- source
        paper suggests a value of 100.

    weighting_function : callable
        Function that computes a weight from a pixel intensity

    Returns
    -------
    numpy.ndarray, dtype=np.float64
        Return a vector g(z) where the element at index i is the log exposure
        of a pixel with intensity value z = i (e.g., g[0] is the log exposure
        of z=0, g[1] is the log exposure of z=1, etc.)
    """
    z_min, z_max = 0, 255
    intensity_range = 255  # difference between min and max possible pixel value for uint8
    num_samples = intensity_samples.shape[0]
    num_images = len(log_exposures)

    # NxP + [(Zmax-1) - (Zmin + 1)] + 1 constraints; N + 256 columns
    mat_A = np.zeros((num_images * num_samples + intensity_range, num_samples + intensity_range + 1), dtype=np.float32)
    mat_b = np.zeros((mat_A.shape[0], 1), dtype=np.float32)

    # 1. Add data-fitting constraints:
    k = 0
    for i in range(num_samples):
        for j in range(num_images):
            z_ij = intensity_samples[i, j]
            w_ij = weighting_function(z_ij)
            mat_A[k, z_ij] = w_ij
            mat_A[k, (intensity_range + 1) + i] = -w_ij
            mat_b[k, 0] = w_ij * log_exposures[j]
            k += 1

    # 2. Add smoothing constraints:
    for z_k in range(z_min + 1, z_max):
        w_k = weighting_function(z_k)
        mat_A[k, z_k - 1] = w_k * smoothing_lambda
        mat_A[k, z_k    ] = -2 * w_k * smoothing_lambda
        mat_A[k, z_k + 1] = w_k * smoothing_lambda
        k += 1

    # 3. Add color curve centering constraint:
    mat_A[k, (z_max - z_min) // 2] = 1

    inv_A = np.linalg.pinv(mat_A)
    x = np.dot(inv_A, mat_b)

    g = x[0: intensity_range + 1]
    return g[:, 0]


def computeRadianceMap(images, log_exposure_times, response_curve, weighting_function):
    """Calculate a radiance map for each pixel from the response curve.

    Parameters
    ----------
    images : list
        Collection containing a single color layer (i.e., grayscale)
        from each image in the exposure stack. (size == num_images)

    log_exposure_times : numpy.ndarray
        Array containing the log exposure times for each image in the
        exposure stack (size == num_images)

    response_curve : numpy.ndarray
        Least-squares fitted log exposure of each pixel value z

    weighting_function : callable
        Function that computes the weights

    Returns
    -------
    numpy.ndarray(dtype=np.float64)
        The image radiance map (in log space)
    """
    img_shape = images[0].shape
    img_rad_map = np.zeros(img_shape, dtype=np.float32)
#FÜR ALLE PIXEL IN EINEM ARRAY
    num_images = len(images)
    for i in range(img_shape[0]):
        for j in range(img_shape[1]): #für jedes bild k, jeden pixel
            g = np.array([response_curve[images[k][i, j]] for k in range(num_images)])
            w = np.array([weighting_function(images[k][i, j]) for k in range(num_images)])
            SumW = np.sum(w)
            if SumW > 0:
                img_rad_map[i, j] = np.sum(w * (g - log_exposure_times) / SumW)
            else:
                img_rad_map[i, j] = g[num_images // 2] - log_exposure_times[num_images // 2] #lg([50,100,150,250,400,650,1050,1700,2750])
    return img_rad_map


#IMAGE SEGMENTATION
def intensityAdjustment(image, template):
    """Tune image intensity based on template
        ----------
        images : <numpy.ndarray>
            image needed to be adjusted
        template : <numpy.ndarray>
            Typically we use the middle image from image stack. We want to match the image
            intensity for each channel to template's
        Returns
        -------
        numpy.ndarray
            The resulting image after intensity adjustment
        """
    m, n, channel = image.shape
    output = np.zeros((m, n, channel))
    for ch in range(channel):
        image_avg, template_avg = np.average(image[:, :, ch]), np.average(template[:, :, ch])
        output[..., ch] = image[..., ch] * (template_avg / image_avg)

    return output

def computeHDR(images, log_exposure_times, smoothing_lambda=100., gamma=1):
    """Computational pipeline to produce the HDR images
    ----------
    images : list<numpy.ndarray>
        A list containing an exposure stack of images
    log_exposure_times : numpy.ndarray
        The log exposure times for each image in the exposure stack
    smoothing_lambda : np.int (Optional)
        A constant value to correct for scale differences between
        data and smoothing terms in the constraint matrix -- source
        paper suggests a value of 100.
    Returns
    -------
    numpy.ndarray
        The resulting HDR with intensities scaled to fit uint16 range

        Normalize relativ for images view by human
        Normalize MinMax for images computer vision
    """

    num_channels = images[0].shape[2]
    hdr_image = np.zeros(images[0].shape, dtype=np.float64)
    response_curve_list = []
    img_rad_map_list = []

    for channel in range(num_channels):
        # Collect the current layer of each input image from the exposure stack
        layer_stack = [img[:, :, channel] for img in images] #(9,1014,760,3) [0 255]

        # Sample image intensities
        intensity_samples = sampleIntensities(layer_stack)

        # Compute Response Curve
        response_curve = computeResponseCurve(intensity_samples, log_exposure_times, smoothing_lambda, linearWeight)
        response_curve_list.append(response_curve)

        # Build radiance map
        img_rad_map = computeRadianceMap(layer_stack, log_exposure_times, response_curve, linearWeight)
        img_rad_map_list.append(img_rad_map)

        # Normalize hdr layer to (0, 4095)
        #hdr_image[..., channel] = cv2.normalize(img_rad_map, None, alpha=0, beta=4095, norm_type=cv2.NORM_MINMAX)
        hdr_image[..., channel] = cv2.normalize(img_rad_map, None, alpha=0, beta=65535, norm_type=cv2.NORM_MINMAX, dtype=cv2.CV_16U) #solarconstant 1361 W/m² extraterr. radiance
        

    # if len(image_mapped.shape) < 3:   # NOTE THIS CODE IS A MODIFICATION 
    #   image_mapped  = image_mapped[:,:,np.newaxis]

    # Adjust image intensity based on the middle image from image stack
    #template = images[len(images)//2]

    mean_ind_0, mean_val_0, mean_list_0 = get_mean_ind(images, # image array with all 9 images
                                                      0)       # index of the channel

    mean_ind_1, mean_val_1, mean_list_1 = get_mean_ind(images, # image array with all 9 images
                                                      1)       # index of the channel

    mean_ind_2, mean_val_2, mean_list_2 = get_mean_ind(images, # image array with all 9 images
                                                      2)       # index of the channel

    mean_ch_list = [[mean_list_0, mean_val_0, mean_ind_0], [mean_list_1, mean_val_1, mean_ind_1], [mean_list_2, mean_val_2, mean_ind_0]]


    if (mean_ind_0 == mean_ind_1) and (mean_ind_1 == mean_ind_2) and (mean_ind_0 == mean_ind_2):
      print("Mean index for all channels : ",mean_ind_1)
    else:
      print("There is a mismatch in mean index amongst channels")
      print("channel 0 mean index ", mean_ind_0)
      print("channel 1 mean index ", mean_ind_1)
      print("channel 2 mean index ", mean_ind_2)

    template = images[mean_ind_1]
    
    image_tuned = intensityAdjustment(hdr_image, template) #returns output
    output = cv2.normalize(image_tuned, None, alpha=0, beta=1361, norm_type=cv2.NORM_MINMAX, dtype=cv2.CV_16U) #600x450=270.000

    output_p = cv2.normalize(image_tuned, None, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX, dtype=cv2.CV_8U)
    output_preview = img_as_ubyte(exposure.rescale_intensity(output_p))

    return response_curve_list,img_rad_map_list,hdr_image.astype(np.uint16),mean_ch_list,output.astype(np.uint16),output_preview.astype(np.uint8)

def get_merg_from_img_array(img_list_arr, exposure_list, output_tuned, save_path, ts):

  """
  This method takes stacked image array and corresponding exposure list as an input and merge the image files and save the output results

  img_list_arr  = array containg stack of image arrays (merge input)
  exposure_list = list of exposure time for all the images in the img_list_arr (oder of this list should be corresponding to the arrays in the img_list_arr)
  output_tuned = if tuned images should also be saved then set this flag to True, otherwise False

  returns None
  """
  ret_dict = {}

  rc_list, irm_list, hdr, mean_ch_list, output, output_preview = computeHDR(img_list_arr, exposure_list, smoothing_lambda=100., gamma=1)     # Merging : It returns output and tuned arrays

  np.save(os.path.join(save_path, "crf_npy")+"/"+str(ts)+'_rc.npy',np.array(rc_list))               #float64
  np.save(os.path.join(save_path, "radmap_npy")+"/"+str(ts)+'_radmap.npy',np.array(irm_list))       #float64
  np.save(os.path.join(save_path, "merge_npy")+"/"+str(ts)+'hdr.npy',hdr)                           #uint16
  
  if output_tuned:
    np.save(os.path.join(save_path, "solar_npy")+"/"+str(ts)+'.npy',output)                   #solarconstant 1361W/m² 
    cv2.imwrite(os.path.join(save_path, "preview_png",str(ts)+'.png'), output_preview)        #uint8 relative 
  
  return mean_ch_list

"""## PARAMS"""

save_path = "/content/drive/MyDrive/DATASETS/LA/original/2022/12/24_merg/2_Test"  # Base path to save all the output files (it requires certain directories to be inside it)
path = "/content/drive/MyDrive/DATASETS/LA/original/2022/12/" 

#save_path = "/content/drive/MyDrive/Upwork/Paul_M/DATASETS/LA/original/2022/12/24_merg/1_Test"  # Base path to save all the output files (it requires certain directories to be inside it)
#path =      "/content/drive/MyDrive/Upwork/Paul_M/DATASETS/LA/original/2022/12/" 

data_path = os.path.join(path, "24")        # Path to the array folder
folder_name = "24"                           # Name of the folder with all the images
file_format = ".tga"                                  # extenstion of the image files 
chunk_size = 9                                        # No of images in one chunk

#Extracting all the paths and exposure list in a list
img_path_list = [[int(f.split("/")[-1].split("_")[0]), int(f.split("/")[-1].split("_")[1].split(".")[0]), os.path.join(data_path,f)] for f in os.listdir(os.path.join(data_path)) if file_format in f]
s_df = pd.DataFrame(img_path_list, columns = ["time_stamp", "exposure_time","img_path"]) # Saving files into data frame
s_df.sort_values(by = ['time_stamp', 'exposure_time'], inplace = True)                                    # Sorting files by timestamo unix
s_df.head()

# Commented out IPython magic to ensure Python compatibility.
# %%time
# # exp_time = [50,100,150,250,400,650,1050,1700,2750] ************** DO NOT USE THIS FOR TGA BECAUSE EXPOSURE TIME IS IN THE NAME OF THE FILE NOT DEFINED EXPLICITILY
# 
# excluded_ts_list = []                            # This list contains all timestamps that contains images less than chunk size 
# template_dict = {}
# resize_height = 450
# resize_width = 600
# radius = 353*(1014/resize_width)
# x = 574*(1014/resize_width)
# y = 335*(1014/resize_width)
# centre = (x,y)
# for ts in s_df.time_stamp.unique().tolist()[:1]: # IMPORTANT NOTE : use unique_sorted_ts[:some_small_int] to see this working because it takes alot of time to run it for entire list
#     
#     chunk_df = s_df[s_df["time_stamp"] == ts]
#     if chunk_df.shape[0] >= chunk_size:                                                                         # Validating for the timestamps that contains minimum no of frames to be merged
#       c_df = chunk_df.iloc[0:chunk_size]
#       img_list_arr = np.array([resize_image_by_shape(np.asarray(Image.open(p)), width = resize_width , height = resize_height, radius = radius, centre = centre) for p in c_df.img_path.tolist()])                      # Reading all images into a list and converting it to numpy array
#       exposure_list = np.log(c_df.exposure_time.tolist())
#       temp_dict = get_merg_from_img_array(img_list_arr,     # Array containg stack of image arrays (merge input)
#                                           exposure_list,    # log of the exposure time for all the corresponding images
#                                           True,             # Flag to save/not save the tuned image
#                                           save_path,        # Base path where all the saving directories are located
#                                           ts)               # Unix timestamp for naming
#       template_dict[ts] = [temp_dict, c_df.exposure_time.tolist()]
#     else:
#       excluded_ts_list.append(ts)                                               # Saving the exluded timestamp with non sufficient amount of frames in it
#       print("Time stamp contains less image frames than chunk size :"+str(ts))

pickle_file = "template_dict.pickle"
template_dict_path = os.path.join(save_path, pickle_file) # Path at which template dictionary will be saved as pickle 
print("template dictionary will be saved at :"+template_dict_path)
save_dict(template_dict_path, template_dict)
dict_temp = read_dict(template_dict_path)
for ts in dict_temp:
  plot_list_bar_all(dict_temp[ts][0], dict_temp[ts][1], ts)

# Commented out IPython magic to ensure Python compatibility.
# '''
# %%time
# exp_time = [60,106,166,273,470,773,1258,2032,3291] # Exposure time for all 9 images read from .npz file
# excluded_ts_list = []                              # This list contains all timestamps that contains images less than chunk size 
# template_dict = {}
# for arr_path in [s_df.arr_path.tolist()[350]]: # REMOVE THE OUTER BRACES AS WELL
#   chunk_arr = np.load(arr_path)
#   img_list_arr = np.array([resize_image_by_shape(cv2.cvtColor(chunk_arr["arr_"+str(i)].view(np.uint16), cv2.COLOR_BayerRGGB2RGB), width = 60,height= 30) for i in range(chunk_size)])
# 
#   print(chunk_arr["arr_"+str(i)].view(np.uint).shape for i in range(chunk_size))
#   file_stats = os.stat(arr_path)
#   print(f'File Size in MegaBytes is {file_stats.st_size / (1024 * 1024)}')
#   print(img_list_arr.shape)
#   ts = arr_path.split("/")[-1].split(".")[0]
#   if img_list_arr.shape[0] >= chunk_size:
#     # Merging the images
#     temp_dict = get_merg_from_img_array(img_list_arr,     # Array containg stack of image arrays (merge input)
#                                         np.log(exp_time), # log of the exposure time for all the corresponding images
#                                         False,            # Flag to save/not save the tuned image
#                                         save_path,        # Base path where all the saving directories are located
#                                         ts)               # Unix timestamp for naming
#     template_dict[ts] = temp_dict
#   else:
#     excluded_ts_list.append(ts)                                               # Saving the exluded timestamp with non sufficient amount of frames in it
#     print("Time stamp contains less image frames than chunk size :"+str(ts))
#     '''

import datetime
import time
from datetime import timezone, timedelta, datetime
from time import sleep

dt = datetime.now(timezone.utc)
utc_time = dt.replace(tzinfo=timezone.utc)
utc_timestamps = str(utc_time)

SS = datetime.now(timezone.utc).second
MM = datetime.now(timezone.utc).minute
HH = datetime.now(timezone.utc).hour
DD = datetime.now(timezone.utc).day
MM = datetime.now(timezone.utc).month
YYYY = datetime.now(timezone.utc).year

YYYYs = str(YYYY)
MMs = str(MM)
DDs = str(DD)
